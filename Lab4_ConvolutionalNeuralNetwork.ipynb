{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "29b9bd1d-766f-4422-ad96-de0accc1ce58"
    }
   },
   "source": [
    "# Lab 4 - Convolutional Neural Network with MNIST\n",
    "\n",
    "This lab corresponds to Module 4 of the \"Deep Learning Explained\" course. We assume that you have successfully Lab 1 to download the MNIST data.\n",
    "\n",
    "We will train a Convolutional Neural Network (CNN) on MNIST data. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "A [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN, or ConvNet) is a type of [feed-forward](https://en.wikipedia.org/wiki/Feedforward_neural_network) artificial neural network made up of neurons that have learnable weights and biases, very similar to ordinary multi-layer perceptron (MLP) networks introduced in Module 3. The CNNs take advantage of the spatial nature of the data. \n",
    "\n",
    "In nature, we perceive different objects by their shapes, size and colors. For example, objects in a natural scene are typically composed of edges, corners/vertices (defined by two of more edges), color patches etc. These primitives are often identified using different detectors (e.g., edge detection, color detector) or combination of detectors interacting to facilitate image interpretation (object classification, region of interest detection, scene description etc.) in real world vision related tasks. These detectors are also known as filters. \n",
    "\n",
    "Convolution is a mathematical operator that takes an image and a filter as input and produces a filtered output (representing say egdges, corners, colors etc in the input image).  Historically, these filters are a set of weights that were often hand crafted or modeled with mathematical functions (e.g., [Gaussian](https://en.wikipedia.org/wiki/Gaussian_filter) / [Laplacian](http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm) / [Canny](https://en.wikipedia.org/wiki/Canny_edge_detector) filter).  The filter outputs are mapped through non-linear activation functions mimicking human brain cells called [neurons](https://en.wikipedia.org/wiki/Neuron).\n",
    "\n",
    "Convolutional networks provide a machinery to learn these filters from the data directly instead of explicit mathematical models and have been found to be superior (in real world tasks) compared to historically crafted filters.  With convolutional networks, the focus is on learning the filter weights instead of learning individually fully connected pair-wise (between inputs and outputs) weights. In this way, the number of weights to learn is reduced when compared with the traditional MLP networks from the previous tutorials.  In a convolutional network, one learns several filters ranging in number from single digits to thousands depending on the network complexity.\n",
    "\n",
    "Many of the CNN primitives have been shown to have a conceptually parallel components in brain's [visual cortex](https://en.wikipedia.org/wiki/Visual_cortex). A neuron in the visual cortex will emit responses when a certain region of its input cells are stimulated. This region is known as the receptive field (RF) of the neuron. \n",
    "\n",
    "Equivalently, in a convolution layer, the input region corresponding to the filter dimensions at certain locations in the input layer can be considered as the receptive field of the nodes in the convolutional layer. Popular deep CNNs or ConvNets (such as [AlexNet](https://en.wikipedia.org/wiki/AlexNet), [VGG](https://arxiv.org/abs/1409.1556), [Inception](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf), [ResNet](https://arxiv.org/pdf/1512.03385v1.pdf)) that are used for various [computer vision](https://en.wikipedia.org/wiki/Computer_vision) tasks have many of these architectural primitives (inspired from biology).  \n",
    "\n",
    "We will introduce the convolution operation and gain familiarity with the different parameters in CNNs.\n",
    "\n",
    "**Problem**:\n",
    "We will continue to work on the same problem of recognizing digits in MNIST data. The MNIST data comprises of hand-written digits with little background noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://3.bp.blogspot.com/_UpN7DfJA0j4/TJtUBWPk0SI/AAAAAAAAABY/oWPMtmqJn3k/s1600/mnist_originals.png\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure 1\n",
    "Image(url= \"http://3.bp.blogspot.com/_UpN7DfJA0j4/TJtUBWPk0SI/AAAAAAAAABY/oWPMtmqJn3k/s1600/mnist_originals.png\", width=200, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**:\n",
    "Our goal is to train a classifier that will identify the digits in the MNIST dataset. \n",
    "\n",
    "**Approach**:\n",
    "\n",
    "The same 5 stages we have used in the previous labs are applicable: Data reading, Data preprocessing, Creating a model, Learning the model parameters and Evaluating (a.k.a. testing/prediction) the model. \n",
    "\n",
    "We will experiment with two models with different architechtural components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "138d1a78-02e2-4bd6-a20e-07b83f303563"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'cntk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e18bc270d56d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mcntk\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'cntk'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cntk as C\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the block below, we check if we are running this notebook in the CNTK internal test machines by looking for environment variables defined there. We then select the right target device (GPU vs CPU) to test this notebook. In other cases, we use CNTK's default policy to use the best available device (GPU, if available, else CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select the right target device when this notebook is being tested:\n",
    "if 'TEST_DEVICE' in os.environ:\n",
    "    if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "        C.device.try_set_default_device(C.device.cpu())\n",
    "    else:\n",
    "        C.device.try_set_default_device(C.device.gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test for CNTK version\n",
    "if not C.__version__ == \"2.0\":\n",
    "    raise Exception(\"this notebook is designed to work with 2.0. Current Version: \" + C.__version__) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading\n",
    "In this section, we will read the data generated in Lab1_MNIST_DataLoader.\n",
    "\n",
    "We are using the MNIST data that you have downloaded using the Lab1_MNIST_DataLoader notebook. The dataset has 60,000 training images and 10,000 test images with each image being 28 x 28 pixels. Thus the number of features is equal to 784 (= 28 x 28 pixels), 1 per pixel. The variable `num_output_classes` is set to 10 corresponding to the number of digits (0-9) in the dataset.\n",
    "\n",
    "In previous labs, as shown below, we have always flattened the input image into a vector.  With convoultional networks, we do **not** flatten the image - we preserve its 2D shape..\n",
    "\n",
    "**Input Dimensions**:  \n",
    "\n",
    "In convolutional networks for images, the input data is often shaped as a 3D matrix (number of channels, image width, image height), which preserves the spatial relationship between the pixels. In the MNIST data, the image is a single channel (grayscale) data, so the input dimension is specified as a (1, image width, image height) tuple. \n",
    "\n",
    "![input-rgb](https://www.cntk.ai/jup/cntk103d_rgb.png)\n",
    "\n",
    "Natural scene color images are often presented as Red-Green-Blue (RGB) color channels. The input dimension of such images are specified as a (3, image width, image height) tuple. If one has RGB input data as a volumetric scan with volume width, volume height and volume depth representing the 3 axes, the input data format would be specified by a tuple of 4 values (3, volume width, volume height, volume depth). In this way CNTK enables specification of input images in arbitrary higher-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure we always get the same amount of randomness\n",
    "np.random.seed(0)\n",
    "C.cntk_py.set_fixed_random_seed(1)\n",
    "C.cntk_py.force_deterministic_algorithms()\n",
    "\n",
    "# Define the data dimensions\n",
    "input_dim_model = (1, 28, 28)    # images are 28 x 28 with 1 channel of color (gray)\n",
    "input_dim = 28*28                # used by readers to treat input data as a vector\n",
    "num_output_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading\n",
    "\n",
    "There are different ways one can read data into CNTK. The easiest way is to load the data in memory using NumPy / SciPy / Pandas readers. However, this can be done only for small data sets. Since deep learning requires large amount of data we have chosen in this course to show how to leverage built-in distributed readers that can scale to terrabytes of data with little extra effort. \n",
    "\n",
    "We are using the MNIST data you have downloaded using Lab 1 DataLoader notebook. The dataset has 60,000 training images and 10,000 test images with each image being 28 x 28 pixels. Thus the number of features is equal to 784 (= 28 x 28 pixels), 1 per pixel. The variable `num_output_classes` is set to 10 corresponding to the number of digits (0-9) in the dataset.\n",
    "\n",
    "In Lab 1, the data was downloaded and written to 2 CTF (CNTK Text Format) files, 1 for training, and 1 for testing. Each line of these text files takes the form:\n",
    "\n",
    "    |labels 0 0 0 1 0 0 0 0 0 0 |features 0 0 0 0 ... \n",
    "                                                  (784 integers each representing a pixel)\n",
    "    \n",
    "We are going to use the image pixels corresponding the integer stream named \"features\". We define a `create_reader` function to read the training and test data using the [CTF deserializer](https://cntk.ai/pythondocs/cntk.io.html?highlight=ctfdeserializer#cntk.io.CTFDeserializer). The labels are [1-hot encoded](https://en.wikipedia.org/wiki/One-hot). Refer to Lab 1 for data format visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    \n",
    "    ctf = C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "          labels=C.io.StreamDef(field='labels', shape=num_label_classes, is_sparse=False),\n",
    "          features=C.io.StreamDef(field='features', shape=input_dim, is_sparse=False)))\n",
    "                          \n",
    "    return C.io.MinibatchSource(ctf,\n",
    "        randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory is data\\MNIST\n"
     ]
    }
   ],
   "source": [
    "# Ensure the training and test data is available\n",
    "# We search in two locations in the toolkit for the cached MNIST data set.\n",
    "\n",
    "data_found=False # A flag to indicate if train/test data found in local cache\n",
    "for data_dir in [os.path.join(\"..\", \"Examples\", \"Image\", \"DataSets\", \"MNIST\"),\n",
    "                 os.path.join(\"data\", \"MNIST\")]:\n",
    "    \n",
    "    train_file=os.path.join(data_dir, \"Train-28x28_cntk_text.txt\")\n",
    "    test_file=os.path.join(data_dir, \"Test-28x28_cntk_text.txt\")\n",
    "    \n",
    "    if os.path.isfile(train_file) and os.path.isfile(test_file):\n",
    "        data_found=True\n",
    "        break\n",
    "        \n",
    "if not data_found:\n",
    "    raise ValueError(\"Please generate the data by completing Lab1_MNIST_DataLoader\")\n",
    "    \n",
    "print(\"Data directory is {0}\".format(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Model Creation'></a>\n",
    "## CNN Model Creation\n",
    "\n",
    "CNN is a feedforward network made up of bunch of layers in such a way that the output of one layer becomes the input to the next layer (similar to MLP). In MLP, all possible pairs of input pixels are connected to the output nodes with each pair having a weight, thus leading to a combinatorial explosion of parameters to be learnt and also increasing the possibility of overfitting ([details](http://cs231n.github.io/neural-networks-1/)). Convolution layers take advantage of the spatial arrangement of the pixels and learn multiple filters that significantly reduce the amount of parameters in the network ([details](http://cs231n.github.io/convolutional-networks/)). The size of the filter is a parameter of the convolution layer.  \n",
    "\n",
    "In this section, we introduce the basics of convolution operations. We show the illustrations in the context of RGB images (3 channels), eventhough the MNIST data we are using a grayscale image (single channel).\n",
    "\n",
    "![input-rgb](https://www.cntk.ai/jup/cntk103d_rgb.png)\n",
    "\n",
    "### Convolution Layer\n",
    "\n",
    "A convolution layer is a set of filters. Each filter is defined by a weight (**W**) matrix, and  bias ($b$).\n",
    "\n",
    "![input-filter](https://www.cntk.ai/jup/cntk103d_filterset_v2.png)\n",
    "\n",
    "These filters are scanned across the image performing the dot product between the weights and corresponding input value (${x}$). The bias value is added to the output of the dot product and the resulting sum is optionally mapped through an activation function. This process is illustrated in the following animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.cntk.ai/jup/cntk103d_conv2d_final.gif\" width=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://www.cntk.ai/jup/cntk103d_conv2d_final.gif\", width= 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution layers incorporate the following key features:\n",
    "\n",
    "   - Instead of being fully-connected to all input nodes , each convolution node is **locally-connected** to a subset of input nodes localized to a smaller input region, also referred to as receptive field (RF). The figure above illustrates a small 3 x 3 regions in the image as the RF region. In the case of an RGB, image there would be 3 such 3 x 3 regions, one for each of the 3 color channels. \n",
    "   \n",
    "   \n",
    "   - Instead of having a single set of weights (as in a Dense layer), convolutional layers have multiple sets (shown in figure with multiple colors), called **filters**. Each filter detects features within each possible RF in the input image.  The output of the convolution is a set of `n` sub-layers (shown in the animation below) where `n` is the number of filters (refer to the above figure).  \n",
    "   \n",
    "     \n",
    "   - Within a sublayer, instead of each node having its own set of weights, a single set of **shared weights** are used by all nodes in that sublayer. This reduces the number of parameters to be learnt and help reduce the risk of overfitting. This also opens the door for several aspects of deep learning which has enabled very practical solutions to be built:\n",
    "    - Handling larger images (say 512 x 512)\n",
    "    - Trying larger filter sizes (corresponding to a larger RF) say 11 x 11\n",
    "    - Learning more filters (say 128)\n",
    "    - Explore deeper architectures (100+ layers)\n",
    "    - Achieve translation invariance (the ability to recognize a feature independent of where they appear in the image). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strides and Pad parameters\n",
    "\n",
    "**How are filters positioned?** In general, the filters are arranged in overlapping tiles, from left to right, and top to bottom.  Each convolution layer has a parameter to specify the `filter_shape`, specifying the width and height of the filter.  There is a parameter (`strides`) that controls the how far to step to right when moving the filters through multiple RF's in a row, and how far to step down when moving to the next row.  The boolean parameter `pad` controls if the input should be padded around the edges to allow a complete tiling of the RF's near the borders. \n",
    "\n",
    "The animation above shows the results with a `filter_shape` = (3, 3), `strides` = (2, 2) and `pad` = False. The two animations below show the results when `pad` is set to True. First, with a stride of 2 and second having a stride of 1.\n",
    "Note: the shape of the output (the teal layer) is different between the two stride settings. In many problems, the stride and pad values are chosen to control the size of output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With stride = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.cntk.ai/jup/cntk103d_padding_strides.gif\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With stride = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.cntk.ai/jup/cntk103d_same_padding_no_strides.gif\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot images with strides of 2 and 1 with padding turned on\n",
    "images = [(\"https://www.cntk.ai/jup/cntk103d_padding_strides.gif\" , 'With stride = 2'),\n",
    "          (\"https://www.cntk.ai/jup/cntk103d_same_padding_no_strides.gif\", 'With stride = 1')]\n",
    "\n",
    "for im in images:\n",
    "    print(im[1])\n",
    "    display(Image(url=im[0], width=200, height=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our CNN models\n",
    "\n",
    "We define two containers. One for the input MNIST image and the second one being the labels corresponding to the 10 digits. When reading the data, the reader automatically maps the 784 pixels per image to a shape defined by the `input_dim_model` tuple (in this example it is set to (1, 28, 28))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = C.input_variable(input_dim_model)\n",
    "y = C.input_variable(num_output_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we build is a simple convolution only network. Here we have two convolutional layers. Since, our task is to detect the 10 digits in the MNIST database, the output of the network should be a vector of length 10, 1 element corresponding to each output class. This is achieved by projecting the output of the last convolutional layer using a dense layer with the output being `num_output_classes`. We have seen this before with Logistic Regression and MLP where features were mapped to the number of classes in the final layer. Also, note that since we will be using the `softmax` operation that is combined with the `cross entropy` loss function during training (see a few cells below), the final dense layer has no activation function associated with it.\n",
    "\n",
    "The following figure illustrates the model we are going to build. Note the parameters in the model below are to be experimented with. These are often called network hyperparameters. Increasing the filter shape leads to an increase in the number of model parameters, increases the compute time and helps the model better fit to the data. However, one runs the risk of [overfitting](https://en.wikipedia.org/wiki/Overfitting). Typically, the number of filters in the deeper layers are more than the number of filters in the layers before them. We have chosen 8, 16 for the first and second layers, respectively. These hyperparameters should be experimented with during model building.\n",
    "\n",
    "![conv-only](https://www.cntk.ai/jup/cntk103d_convonly2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to build model\n",
    "\n",
    "def create_model(features):\n",
    "    with C.layers.default_options(init=C.glorot_uniform(), activation=C.relu):\n",
    "            h = features\n",
    "            h = C.layers.Convolution2D(filter_shape=(5,5), \n",
    "                                       num_filters=8, \n",
    "                                       strides=(1, 1), \n",
    "                                       pad=True, name='first_conv')(h)\n",
    "            h = C.layers.AveragePooling((5,5), strides=(2,2), pad=False, name='first_pool')(h)\n",
    "            h = C.layers.Convolution2D(filter_shape=(5,5), \n",
    "                                       num_filters=16, \n",
    "                                       strides=(1,1), \n",
    "                                       pad=True, name='second_conv')(h)\n",
    "            h = C.layers.AveragePooling((5,5), strides=(2,2), pad=False, name='second_pool')(h)\n",
    "            r = C.layers.Dense(num_output_classes, activation=None, name='classify')(h)\n",
    "            return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create an instance of the model and inspect the different components of the model. `z` will be used to represent the output of a network. In this model, we use the `relu` activation function. Note: using the `C.layers.default_options` is an elegant and concise way to build models. This is key to minimizing modeling errors, saving precious debugging time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape of the first convolution layer: (8, 28, 28)\n",
      "Bias value of the last dense layer: [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "z = create_model(x)\n",
    "\n",
    "# Print the output shapes / parameters of different components\n",
    "print(\"Output Shape of the first convolution layer:\", z.first_conv.shape)\n",
    "print(\"Bias value of the last dense layer:\", z.classify.b.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the number of learnable parameters in a model is key to deep learning since there is a  dependency between the number of parameters and the amount of data one needs to have to train the model. \n",
    "\n",
    "You need more data for a model that has a larger number of parameters to prevent overfitting. In other words, with a fixed amount of data, one has to constrain the number of parameters. There is no golden rule between the amount of data one needs for a model. However, there are ways one can boost performance of model training with [data augmentation](https://deeplearningmania.quora.com/The-Power-of-Data-Augmentation-2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 5994 parameters in 6 parameter tensors.\n"
     ]
    }
   ],
   "source": [
    "# Number of parameters in the network\n",
    "C.logging.log_number_of_parameters(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Parameters**:\n",
    "\n",
    "\n",
    "Our model has 2 convolution layers each having a weight and bias. This adds up to 4 parameter tensors. Additionally the dense layer has weight and bias tensors. Thus, the 6 parameter tensors.\n",
    "\n",
    "Remember that in a convolutional layer, the number of parameters is not dependent on the number of nodes, only on the shared weights and bias of each filter.\n",
    "\n",
    "Let us now count the number of parameters:\n",
    "- *First convolution layer*: There are 8 filters each of size (1 x 5 x 5) where 1 is the number of channels in the input image. This adds up to 200 values in the weight matrix and 8 bias values.   \n",
    "\n",
    "\n",
    "- *Second convolution layer*: There are 16 filters each of size (8 x 5 x 5) where 8 is the number of channels in the input to the second layer (= output of the first layer). This adds up to 3200 values in the weight matrix and 16 bias values.\n",
    "\n",
    "\n",
    "- *Last dense layer*: There are 16 x 7 x 7 input values and it produces 10 output values corresponding to the 10 digits in the MNIST dataset. This corresponds to (16 x 7 x 7) x 10 weight values and 10 bias values.\n",
    "\n",
    "Adding these up gives the 11274 parameters in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge Check**: Does the dense layer shape align with the task (MNIST digit classification)?\n",
    "\n",
    "** Suggested Explorations **\n",
    "- Try printing shapes and parameters of different network layers,\n",
    "- Record the training error you get with `relu` as the activation function,\n",
    "- Now change to `sigmoid` as the activation function and see if you can improve your training error.\n",
    "- Different supported activation functions can be [found here][]. Which activation function gives the least training error?\n",
    "\n",
    "[found here]: https://github.com/Microsoft/CNTK/wiki/Activation-Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning model parameters\n",
    "\n",
    "We use the `softmax` function to map the accumulated evidences or activations to a probability distribution over the classes (Details of the [softmax function][] and other [activation][] functions).\n",
    "\n",
    "[softmax function]: http://cntk.ai/pythondocs/cntk.ops.html#cntk.ops.softmax\n",
    "\n",
    "[activation]: https://github.com/Microsoft/CNTK/wiki/Activation-Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We minimize the cross-entropy between the label and predicted probability by the network. Since we are going to build more than one model, we will create a few helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_criterion_function(model, labels):\n",
    "    loss = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error(model, labels)\n",
    "    return loss, errs # (model, labels) -> (loss, error metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will need a helper function to perform the model training. First let us create additional helper functions that will be needed to visualize different functions associated with training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a utility function to compute the moving average sum.\n",
    "# A more efficient implementation is possible with np.cumsum() function\n",
    "def moving_average(a, w=5):\n",
    "    if len(a) < w:\n",
    "        return a[:]    # Need to send a copy of the array\n",
    "    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n",
    "\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "    if mb%frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        if verbose: \n",
    "            print (\"Minibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100))\n",
    "        \n",
    "    return mb, training_loss, eval_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure training\n",
    "\n",
    "Previously we have described the concepts of `loss` function, the optimizers or [learners](https://cntk.ai/pythondocs/cntk.learners.html) and the associated machinery needed to train a model. Please refer to earlier labs for gaining familiarility with these concepts. Here we combine model training and testing in a helper function below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test(train_reader, test_reader, model_func, num_sweeps_to_train_with=10):\n",
    "    \n",
    "    # Instantiate the model function; x is the input (feature) variable \n",
    "    # We will scale the input image pixels within 0-1 range by dividing all input value by 255.\n",
    "    model = model_func(x/255)\n",
    "    \n",
    "    # Instantiate the loss and error function\n",
    "    loss, label_error = create_criterion_function(model, y)\n",
    "    \n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    learning_rate = 0.2\n",
    "    lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "    learner = C.sgd(z.parameters, lr_schedule)\n",
    "    trainer = C.Trainer(z, (loss, label_error), [learner])\n",
    "    \n",
    "    # Initialize the parameters for the trainer\n",
    "    minibatch_size = 64\n",
    "    num_samples_per_sweep = 60000\n",
    "    num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / minibatch_size\n",
    "    \n",
    "    # Map the data streams to the input and labels.\n",
    "    input_map={\n",
    "        y  : train_reader.streams.labels,\n",
    "        x  : train_reader.streams.features\n",
    "    } \n",
    "    \n",
    "    # Uncomment below for more detailed logging\n",
    "    training_progress_output_freq = 500\n",
    "     \n",
    "    # Start a timer\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(0, int(num_minibatches_to_train)):\n",
    "        # Read a mini batch from the training data file\n",
    "        data=train_reader.next_minibatch(minibatch_size, input_map=input_map) \n",
    "        trainer.train_minibatch(data)\n",
    "        print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "     \n",
    "    # Print training time\n",
    "    print(\"Training took {:.1f} sec\".format(time.time() - start))\n",
    "    \n",
    "    # Test the model\n",
    "    test_input_map = {\n",
    "        y  : test_reader.streams.labels,\n",
    "        x  : test_reader.streams.features\n",
    "    }\n",
    "\n",
    "    # Test data for trained model\n",
    "    test_minibatch_size = 512\n",
    "    num_samples = 10000\n",
    "    num_minibatches_to_test = num_samples // test_minibatch_size\n",
    "\n",
    "    test_result = 0.0   \n",
    "\n",
    "    for i in range(num_minibatches_to_test):\n",
    "    \n",
    "        # We are loading test data in batches specified by test_minibatch_size\n",
    "        # Each data point in the minibatch is a MNIST digit image of 784 dimensions \n",
    "        # with one pixel per dimension that we will encode / decode with the \n",
    "        # trained model.\n",
    "        data = test_reader.next_minibatch(test_minibatch_size, input_map=test_input_map)\n",
    "        eval_error = trainer.test_minibatch(data)\n",
    "        test_result = test_result + eval_error\n",
    "\n",
    "    # Average of evaluation errors of all test minibatches\n",
    "    print(\"Average test error: {0:.2f}%\".format(test_result*100 / num_minibatches_to_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Run the trainer'></a>\n",
    "### Run the trainer and test model\n",
    "\n",
    "We are now ready to train our convolutional neural net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch: 0, Loss: 2.3210, Error: 89.06%\n",
      "Minibatch: 500, Loss: 0.4868, Error: 20.31%\n",
      "Minibatch: 1000, Loss: 0.1536, Error: 3.12%\n",
      "Minibatch: 1500, Loss: 0.1946, Error: 7.81%\n",
      "Minibatch: 2000, Loss: 0.0599, Error: 3.12%\n",
      "Minibatch: 2500, Loss: 0.0257, Error: 1.56%\n",
      "Minibatch: 3000, Loss: 0.0086, Error: 0.00%\n",
      "Minibatch: 3500, Loss: 0.1308, Error: 4.69%\n",
      "Minibatch: 4000, Loss: 0.0392, Error: 1.56%\n",
      "Minibatch: 4500, Loss: 0.0357, Error: 1.56%\n",
      "Minibatch: 5000, Loss: 0.0691, Error: 1.56%\n",
      "Minibatch: 5500, Loss: 0.0128, Error: 0.00%\n",
      "Minibatch: 6000, Loss: 0.0213, Error: 0.00%\n",
      "Minibatch: 6500, Loss: 0.0484, Error: 1.56%\n",
      "Minibatch: 7000, Loss: 0.0446, Error: 1.56%\n",
      "Minibatch: 7500, Loss: 0.0121, Error: 0.00%\n",
      "Minibatch: 8000, Loss: 0.0217, Error: 1.56%\n",
      "Minibatch: 8500, Loss: 0.0206, Error: 1.56%\n",
      "Minibatch: 9000, Loss: 0.0247, Error: 1.56%\n",
      "Training took 628.7 sec\n",
      "Average test error: 1.06%\n"
     ]
    }
   ],
   "source": [
    "def do_train_test():\n",
    "    global z\n",
    "    z = create_model(x)\n",
    "    reader_train = create_reader(train_file, True, input_dim, num_output_classes)\n",
    "    reader_test = create_reader(test_file, False, input_dim, num_output_classes)\n",
    "    train_test(reader_train, reader_test, z)\n",
    "    \n",
    "do_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the average test error is very comparable to our training error indicating that our model has good \"out of sample\" error a.k.a. [generalization error](https://en.wikipedia.org/wiki/Generalization_error). This implies that our model can very effectively deal with previously unseen observations (during the training process). This is key to avoid [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
    "\n",
    "Let us check what is the value of some of the network parameters. We will check the bias value of the output dense layer. Previously, it was all 0. Now you see there are non-zero values, indicating that a model parameters were updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias value of the last dense layer: [-0.27584177 -0.00497845 -0.07089849 -0.1313996   0.05310194 -0.1490946\n",
      "  0.05973886 -0.22240967  0.83371395 -0.09194217]\n"
     ]
    }
   ],
   "source": [
    "print(\"Bias value of the last dense layer:\", z.classify.b.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation / Prediction\n",
    "We have so far been dealing with aggregate measures of error. Let us now get the probabilities associated with individual data points. For each observation, the `eval` function returns the probability distribution across all the classes. The classifier is trained to recognize digits, hence has 10 classes. First let us route the network output through a `softmax` function. This maps the aggregated activations across the network to probabilities across the 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = C.softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test a small minibatch sample from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data for evaluation\n",
    "reader_eval=create_reader(test_file, False, input_dim, num_output_classes)\n",
    "\n",
    "eval_minibatch_size = 25\n",
    "eval_input_map = {x: reader_eval.streams.features, y:reader_eval.streams.labels} \n",
    "\n",
    "data = reader_eval.next_minibatch(eval_minibatch_size, input_map=eval_input_map)\n",
    "\n",
    "img_label = data[y].asarray()\n",
    "img_data = data[x].asarray()\n",
    "\n",
    "# reshape img_data to: M x 1 x 28 x 28 to be compatible with model\n",
    "img_data = np.reshape(img_data, (eval_minibatch_size, 1, 28, 28))\n",
    "\n",
    "predicted_label_prob = [out.eval(img_data[i]) for i in range(len(img_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the index with the maximum value for both predicted as well as the ground truth\n",
    "pred = [np.argmax(predicted_label_prob[i]) for i in range(len(predicted_label_prob))]\n",
    "gtlabel = [np.argmax(img_label[i]) for i in range(len(img_label))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label    : [7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5, 4]\n",
      "Predicted: [7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"Label    :\", gtlabel[:25])\n",
    "print(\"Predicted:\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize some of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Label:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQcIAAA1BwgAADUHCAAANQcIAAA35P34FcXTrqMYUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x96a0a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a random image\n",
    "sample_number = 5\n",
    "plt.imshow(img_data[sample_number].reshape(28,28), cmap=\"gray_r\")\n",
    "plt.axis('off')\n",
    "\n",
    "img_gt, img_pred = gtlabel[sample_number], pred[sample_number]\n",
    "print(\"Image Label: \", img_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer\n",
    "\n",
    "Often times, one needs to control the number of parameters especially when having deep networks. For every layer of the convolution layer output (each layer, corresponds to the output of a filter), one can have a pooling layer. Pooling layers are typically introduced to:\n",
    "- Reduce the shape of the current layer (speeding up the network),\n",
    "- Make the model more tolerant to changes in object location in the image. For example, even when a digit is shifted to one side of the image instead of being in the middle, the classifer would perform the classification task well.\n",
    "\n",
    "The calculation on a pooling node is much simpler than a normal feedforward node.  It has no weight, bias, or activation function.  It uses a simple aggregation function (like max or average) to compute its output.  The most commonly used function is \"max\" - a max pooling node simply outputs the maximum of the input values corresponding to the filter position of the input. The figure below shows the input values in a 4 x 4 region. The max pooling window size is 2 x 2 and starts from the top left corner, and uses a stride of 2x2.  The maximum value within the window becomes the output of the region. Every time the model is shifted by the amount specified by the stride parameter (as shown in the figure below) and the maximum pooling operation is repeated. \n",
    "![maxppool](https://cntk.ai/jup/201/MaxPooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative is average pooling, which emits that average value instead of the maximum value. The two different pooling opearations are summarized in the animation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max pooling\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.cntk.ai/jup/c103d_max_pooling.gif\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average pooling\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.cntk.ai/jup/c103d_average_pooling.gif\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot images with strides of 2 and 1 with padding turned on\n",
    "images = [(\"https://www.cntk.ai/jup/c103d_max_pooling.gif\" , 'Max pooling'),\n",
    "          (\"https://www.cntk.ai/jup/c103d_average_pooling.gif\", 'Average pooling')]\n",
    "\n",
    "for im in images:\n",
    "    print(im[1])\n",
    "    display(Image(url=im[0], width=200, height=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical convolution network\n",
    "\n",
    "![mnist-conv-mp](http://www.cntk.ai/jup/conv103d_mnist-conv-mp.png)\n",
    "\n",
    "A typical CNN contains a set of alternating convolution and pooling layers followed by a dense output layer for classification. You will find variants of this structure in many classical deep networks (VGG, AlexNet etc).  This is in contrast to the MLP network we used in Lab 3, which consisted of 2 dense layers followed by a dense output layer.  \n",
    "\n",
    "The illustrations are presented in the context of 2-dimensional (2D) images, but the concept and the CNTK components can operate on any dimensional data. The above schematic shows 2 convolution layer and 2 pooling layers. A typical strategy is to increase the number of filters in the deeper layers while reducing the spatial size of each intermediate layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Create a network with Average Pooling\n",
    "\n",
    "Typical convolutional networks have interlacing convolution and pooling layers. The previous model had only convolution layer. In this section, you will create a model with the following architecture.\n",
    "\n",
    "![conv-only](https://www.cntk.ai/jup/cntk103d_conv_max2.png)\n",
    "\n",
    "You will use the CNTK [Average Pooling](https://cntk.ai/pythondocs/cntk.layers.layers.html#cntk.layers.layers.AveragePooling) function to achieve this task. You will edit the `create_model` function below and add the Average Pooling operation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify this model\n",
    "def create_model(features):\n",
    "    with C.layers.default_options(init = C.glorot_uniform(), activation = C.relu):\n",
    "            h = features\n",
    "            \n",
    "            h = C.layers.Convolution2D(filter_shape=(5,5), \n",
    "                                       num_filters=8, \n",
    "                                       strides=(2,2), \n",
    "                                       pad=True, name='first_conv')(h)\n",
    "            h = C.layers.Convolution2D(filter_shape=(5,5), \n",
    "                                       num_filters=16, \n",
    "                                       strides=(2,2), \n",
    "                                       pad=True, name='second_conv')(h)\n",
    "            r = C.layers.Dense(num_output_classes, activation = None, name='classify')(h)\n",
    "            return r\n",
    "        \n",
    "# do_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x9d3d0b8>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYREhCJMCCERoQgTQkhEKMKEEBKR/w/vkau3LyoL6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x97ec080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import PIL\n",
    "\n",
    "imgfile = \"MysteryNumberD (2).bmp\" \n",
    "im = PIL.Image.open(imgfile) \n",
    "\n",
    "p = np.array(im)\n",
    "p = p.astype(np.float32)\n",
    "plt.imshow(p, cmap=\"gray_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = np.reshape(p, [1, 28, 28])\n",
    "p_predict = out.eval(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.71808480e-02   1.90573230e-01   7.40657628e-01   2.75165098e-06\n",
      "    3.02581432e-10   4.66640480e-03   1.90860774e-05   6.89990073e-03\n",
      "    1.39971830e-08   1.36566825e-07]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(p_predict)\n",
    "print(np.argmax(p_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Knowledge Check**: How many parameters do we have in this second model (as shown in the figure)? \n",
    "\n",
    "\n",
    "**Suggested Explorations**\n",
    "- Add average pooling layer after each of the two convolution layer. Use the parameters as shown in the figure. \n",
    "- Does use of LeakyRelu help improve the error rate?\n",
    "- What percentage of the parameter does the last dense layer contribute w.r.t. the overall number of parameters for (a) purely two convolutional layer and (b) alternating 2 convolutional and average pooling layers   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
